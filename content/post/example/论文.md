---
title: "论文"
date: 2021-06-09T14:21:49+08:00
draft: true
---

# 基于TCP长连接的微服务负载均衡器

## 摘要

随着互联网技术的蓬勃发展，庞大的用户和数据量使得分布式和微服务技术成为主流。这就对于高可靠性和实时性、并发性能提出了很高要求，通过更换硬件、水平扩展现有系统受到边际效应递减的影响已经得不偿失。所以急需从软件架构角度优化性能，本文提出采用 TCP 长连接的技术提升通讯层性能，并且对于通过对当前主流负载均衡技术的研究以及测试，我们发现这些技术都不能很好的满足长连接负载均衡的需要，所以本文的研究目标是研究支持 TCP 长连接的负载均衡模型，通过对长连接负载均衡器的设计和开发，解决单台服务器瓶颈和单点故障问题，满足微服务治理的需要。

## 绪论

### 研究背景

随着信息技术的快速演进与不断发展，我们已经进入了互联网信息时代。新硬件计算能力的提高，高速网络新技术的不断扩展，以及软件应用的不断复杂化，业务需求的多样化，技术也在向多元化发展。应用的日益复杂和计算机的分布导致许多互联网应用系统分化为分布式和基于集群的网络架构。随着数据量的大量增加，单个服务器不可能完全处理所有的网络数据和请求。在这种情况下，用于分布式计算和负载平衡的服务器集群的想法诞生了。所谓分布式是指一种理论上的计算模式，数据和程序可以分布在不同的服务器上，分布在网络中的信息数据是主要的研究对象。分布式促进了整个计算机系统上的任务分配和优化，克服了传统集中式系统造成集中式主机资源瓶颈和响应瓶颈的缺点，有助于解决网络中的数据异质性和数据共享问题。分布式实现需要以负载平衡服务器集群的系统架构为基础。负载均衡服务器集群指的是许多服务器的集合，在同一时间运行相同的服务。客户端只需要使用服务器集群提供的同一个服务端口就可以使用该服务，而向客户提供实际服务的主机对外界是透明的。在负载平衡技术的帮助下，服务器可以显著提高并行处理能力。通过集群进行多机备份。即使负载均衡集群中的一台或多台机器出现故障，整个集群系统也能正常运行，确保系统的高可靠性。

业务的蓬勃发展，用户数量的爆发性增长导致了许多互联网应用系统转向基于分布式和集群的微服务架构。微服务架构通过业务拆分实现服务组件化，通过组件组合快速开发系统，业务单一的服务组件又可以独立部署，使得整个系统变得清晰灵活。但是大量的分布式服务又使得架构实现面临问题，如服务注册发现，服务统一接入和权限控制，服务的负载均衡，服务配置的集中管理，服务熔断，服务监控等。所以在微服务架构中，除了业务服务组件外通常会引入服务注册发现组件来进行服务治理。而微服务治理组件实现要求以负载均衡服务器集群的系统架构为基础。

### 研究现状分析

负载均衡主要平衡每个服务节点的负载状态，通过一个相对简单的和易于使用的机制，全面提高系统的整体性能，扩大并充分利用内部资源，并最终保证系统的高可用性和改善外部用户体验。负载均衡有两种含义：首先将大量并发访问或数据流量分派给多个节点设备处理，以减少用户的等待时间。其次将单个重负载计算分派给多个节点设备进行并行处理。在对每个节点设备进行处理后，将结果汇总并返回给用户，提高信息系统的处理能力。负载均衡主要完成以下任务和目标：提高网络利用率，解决网络拥塞问题；保证系统的高可用性，灵活方便的扩展业务节点；降低系统的整体成本，充分利用各种软硬件资源；提高系统稳定性，保证请求成功处理。

目前传统的负载均衡有以下几种实现方式：

1. 基于 DNS 轮询的负载均衡
  DNS 为同一域名配置多个 IP 地址，客户端查询域名时获取其中的一个 IP 地址以实现负载均衡。DNS负载均衡常用于 Web 集群服务
2. 基于代理服务器的负载均衡
  通过代理服务器将请求分发至多台服务器以实现负载均衡。Linux 下可用 LVS 实现代理服务器
3. 基于 NAT 的负载均衡
  网络地址转换( Network Address Translation，NAT)是一种将外部 IP 映射为多个内部 IP 的技术。Linux 内核已包含 NAT 负载均衡功能，通过客户端动态连接一个内部 IP 的方式实现负载均衡

通过对当前主流负载均衡技术的研究以及测试，我们发现这些技术都不能很好的满足长连接负载均衡的需要。本课题将以此为基础进行进一步的扩展研究，完成长连接负载均衡的设计，以满足微服务的实际需求。

### 研究方案

客户端和服务端完成一次消息交互后即断开TCP连接的方式为TCP短连接。多条消息复用同一TCP连接的方式为TCP长连接。长连接可以省去较多的TCP建立/关闭的操作，减少资源消耗和响应时间。但是长连接对于服务器来说较为复杂，需要额外的维护工作。本课题的研究内容是基于TCP长连接的负载均衡器。

采用传统负载均衡模式时，每条TCP长连接仅能接入服务集群中的1台服务器节点，各节点的负载并不均衡，无法动态调整节点数量。所以需要针对长连接的特点设计独特的负载均衡器。为此，本课题研究支持 TCP 长连接的负载均衡模型，通过对长连接负载均衡器的设计和开发，解决单台服务器瓶颈和单点故障问题，满足微服务治理的需要。

本课题要实现的目标如下：

1. 设计基于TCP长连接的通信模型
2. 实现TCP长连接的负载均衡算法
3. 具备连接生命周期实时感知能力，能够根据当前负载主动推送柔性调节

通过本课题的研究预期实现满足生产环境要求，能够支持大规模的请求量和推送量的微服务负载均衡器

## TCP 长连接负载均衡系统简介

### TCP 长连接技术简介

使用TCP协议时，会在客户端和服务器之间建立一条虚拟的信道，这条虚拟信道就是指连接，而建议这条连接需要3次握手，拆毁这条连接需要4次挥手。可见，我们建立这条连接是有成本的，这个成本主要体现在以下三个部分：

1. 时间成本：发送一段数据，必须先 3 次握手即消耗三次 RTT 时间（Round-Trip Time，往返时延），然后才能发送数据；数据发送完毕需要 4 次挥手即消耗四次 RTT 时间来断开这个连接。
2. CPU 资源成本：三次握手和四次挥手以及后续的数据都是从网卡发送和接收的，而且网络通讯并不是简单的数据包的发送与接收还有其余周边设备，比如防火墙、路由器等等。在操作系统内核的角度来讲，对于一个高并发系统，这些操作那是很耗 CPU 资源的。
3. 内存资源成本：建立的每个 socket 是需要耗费系统内存资源的，比如下列 socket 缓存：

```text
/proc/sys/net/ipv4/tcp_rmem
/proc/sys/net/ipv4/tcp_wmem
/proc/sys/net/ipv4/tcp_mem
```

TCP 协议的设计上并没有区分建立的连接是短连接还是长连接。长短与否，完全取决于我们怎么用它：

- 短连接：每次通信时，创建 Socket；一次通信结束，调用 socket.close()。这就是一般意义上的短连接。
- 长连接：每次通信完毕后，不会关闭连接，这样可以做到连接的复用。

短连接的好处是管理起来比较简单，存在的连接都是可用的连接，不需要额外的控制手段。长连接的好处是省去了创建连接的耗时。
短连接和长连接的优势，分别是对方的劣势。想要图简单，不追求高性能，使用短连接合适，这样我们就不需要操心连接状态的管理；想要追求性能，使用长连接，我们就需要担心各种问题：比如端对端连接的维护，连接的保活。

短链接比较通用，广泛的使用在 HTTP 等协议，在 web 场景中尤为常见。

长连接常常被用来做数据的推送，我们大多数时候对通信的认知还是 request/response 模型，但 TCP 双工通信的性质决定了它还可以被用来做双向通信。在长连接之下，可以很方便的实现 push 模型。

### 负载均衡技术简介

负载均衡主要平衡每个服务节点的负载状态，通过一个相对简单的和易于使用的机制，全面提高系统的整体性能，扩大并充分利用内部资源，并最终保证系统的高可用性和改善外部用户体验［1］。负载均衡有两种含义［2］［3］：首先将大量并发访问或数据流量分派给多个节点设备处理，以减少用户的等待时间。其次将单个重负载计算分派给多个节点设备进行并行处理。在对每个节点设备进行处理后，将结果汇总并返回给用户，提高信息系统的处理能力。负载均衡主要完成以下任务和目标：提高网络利用率，解决网络拥塞问题；保证系统的高可用性，灵活方便的扩展业务节点；降低系统的整体成本，充分利用各种软硬件资源；提高系统稳定性，保证请求成功处理。

目前传统的负载均衡有以下几种实现方式：

1. 基于 DNS 轮询的负载均衡
  DNS 为同一域名配置多个 IP 地址，客户端查询域名时获取其中的一个 IP 地址以实现负载均衡［4］。DNS负载均衡常用于 Web 集群服务。
2. 基于代理服务器的负载均衡
  通过代理服务器将请求分发至多台服务器以实现负载均衡。Linux 下可用 LVS 实现代理服务器［5］
3. 基于 NAT 的负载均衡
  网络地址转换( Network Address Translation，NAT)是一种将外部 IP 映射为多个内部 IP 的技术。Linux 内核已包含 NAT 负载均衡功能，通过客户端动态连接一个内部 IP 的方式实现负载均衡［6］

## 长连接负载均衡器设计与实现

### 系统需求和背景

随着业务的蓬勃发展和用户数量的爆发性增长，过去采用的短短连接通信模型给系统背上了沉重的历史负担。性能的瓶颈使得只能通过水平扩容的方法来满足业务的需要。然而水平扩容并不是银弹，随着集群规模的扩大，新增的机器带来的性能提升越来越小，并且集群中机器数量的扩展带来了更多的数据库连接数，给数据库带来了巨大的压力；同时，后端集群规模过于庞大也给负载均衡带来了很大的挑战，为了控制故障的影响范围只能把大集群切割成一个个小集群独立部署，每个小集群都需要部署独立的负载均衡器，这就增进了架构的复杂并且带来了不必要的成本。

所以为了优化性能，解决当前业务发展遇到的瓶颈，我们需要分析短连接模型的不足之处：

1. 资源消耗大：每个请求都建立与断开连接造成系统资源的浪费，并且短连接并没有提供在对端服务不可用时的通知机制，所以只能采用心跳的方式来进行主动探测，而每次心跳探测都是一个短连接。随着集群规模的增大，集群中的每台机器两两之间探测生成巨量的短连接，造成网络资源的极大消耗
2. 时延长：短连接的建立与断开共计消耗高达七个 RTT 的时间，拖慢了业务接口响应时间。并且采用心跳的方式探测对端的存活，两次心跳之间会间隔一段时间，并且心跳在达到超时时间仍然未有响应才会认为对端服务不可用，这种方式时延较长，时效性差。

所以我们需要一种新的连接模型，它能具有以下特性：

1. 持久化连接：避免反复建立、断开连接造成的资源浪费和时延增长
2. 支持双工：可以发送和接收数据
3. 便于管理连接状态：可以及时了解对端的服务状态，可以方便的建立与断开连接

所以我们选用 TCP 长连接来作为通信层的实现以优化性能，但是 TCP 长连接的引入又会带来新的问题：现有的负载均衡模型无法很好的支持长连接，这里以两种常见的负载均衡器为例

- LVS：LVS 是基于 Linux 内核模块提供的能力开发的负载均衡器，属于基于 NAT 的负载均衡模型。也就是说 LVS 是三层负载均衡器，它的原理是修改 ip 报文来进行转发数据。这种负载均衡方式无法感知上层协议的意图，具有粘滞效应：在粘滞时间内只能将报文发给固定的后端服务器，所以它无法支持长时间存在的连接。
- DNS：DNS 负责解析域名和 ip 之间的对应关系，通过对不同的客户端给出不同的服务器地址来实现负载均衡的目的。然而在短连接的连接模型下，每个连接之间都是独立的，也就是说短连接是无状态的。但是对于长连接来说服务端需要管理连接状态、维持连接的存续，也就是说长连接是有状态的，无法使用 DNS 作为负载均衡器。

综上所述，现有的传统负载均衡器各有各的缺陷，无法适应长连接的需要。所以需要研究支持长连接的负载均衡器。

### 系统设计总体目标

根据上一节对系统需求的讨论，总结系统设计总体目标如下：

1. 设计基于TCP长连接的通信模型
2. 实现TCP长连接的负载均衡算法
3. 具备连接生命周期实时感知能力，能够根据当前负载主动推送柔性调节

通过本课题的研究预期实现满足生产环境要求，能够支持大规模的请求量和推送量的微服务负载均衡器

整个系统架构设计图如下：

![Snipaste_2021-05-02_22-53-43]($resource/Snipaste_2021-05-02_22-53-43.png)

其中连接层负责：

1. 负载均衡
2. 流量控制

连接层是实现高可用的基石，负责负载均衡与流量控制

通讯层负责：

1. 连接管理
2. 处理请求
3. 事件推送

通讯层负责实现长连接的管理与请求的处理，以及使用主动推送的方式进行事件通知

### 基于 TCP 长连接的通讯层设计

数据交互有两种模式：Push（推模式）和 Pull（拉模式）

推模式指的是客户端与服务端建立好网络长连接，服务方有相关数据，直接通过长连接通道推送到客户端。其优点是及时，一旦有数据变更，客户端立马能感知到；另外对客户端来说逻辑简单，不需要关心有无数据这些逻辑处理。缺点是不知道客户端的数据消费能力，可能导致数据积压在客户端，来不及处理。

拉模式指的是客户端主动向服务端发出请求，拉取相关数据。其优点是此过程由客户端发起请求，故不存在推模式中数据积压的问题。缺点是可能不够及时，对客户端来说需要考虑数据拉取相关逻辑，何时去拉，拉的频率怎么控制等等。
拉模式有两种实现方式：长轮询（Long Polling）和轮询（Polling）

轮询是指不管服务端数据有无更新，客户端每隔定长时间请求拉取一次数据，可能有更新数据返回，也可能什么都没有。配置中心如果使用轮询实现动态推送，会有以下问题：

- 推送延迟。客户端每隔 5s 拉取一次事件，若事件发生在第 6s，则事件推送的延迟会达到 4s。
- 服务端压力。不一定每次轮询都会有事件发生，频繁的轮询会给服务端造成很大的压力。
- 推送延迟和服务端压力无法中和。降低轮询的间隔，延迟降低，压力增加；增加轮询的间隔，压力降低，延迟增高。

长轮询则不存在上述的问题。客户端发起长轮询，如果服务端的数据没有发生变更，会阻塞住请求，直到服务端的数据发生变化，或者等待一定时间超时才会返回。返回后，客户端又会立即再次发起下一次长轮询。相比轮询，长轮询有如下优势：

- 推送延迟。服务端数据发生变更后，长轮询结束，立刻返回响应给客户端。
- 服务端压力。长轮询的间隔期一般很长，例如 30s、60s，并且服务端阻塞住连接不会消耗太多服务端资源。

长轮询的交互步骤：

1. 客户端发起长轮询
  客户端发起一个 HTTP 请求，若服务端没有数据返回，客户端与服务端之间一直处于连接状态。
2. 服务端监听数据变化
  服务端会维护长轮询的映射关系，如果有数据需要返回，服务端会找到对应的连接，为响应写入数据内容。如果超时内无相应数据产生，服务端找到对应的超时长轮询连接，写入 304 响应。

    > 304 在 HTTP 响应码中代表“未改变”，并不代表错误。比较契合长轮询时，配置未发生变更的场景。

3. 客户端接收长轮询响应
  首先查看响应码是 200 还是 304，以判断配置是否变更，做出相应的回调。之后再次发起下一次长轮询。
4. 服务端设置配置写入的接入点

长轮询依靠超时机制在一定程度上实现 TCP 长连接，但是长轮询需要等待一定时间超时，超时后又发起长轮询，不能让服务端一直保持连接。主要有两个层面的考虑，一是连接稳定性的考虑，长轮询在传输层本质上还是走的 TCP 协议，如果服务端假死、fullgc 等异常问题，或者是重启等常规操作，长轮询没有应用层的心跳机制，仅仅依靠 TCP 层的心跳保活很难确保可用性，所以一次长轮询设置一定的超时时间也是在确保可用性。

为了解决长轮询的缺陷，我们引入 HTTP 2 来代替长轮询中使用的 HTTP 1.1。HTTP 2 是一个全双工的流式协议，相比于 HTTP 1.1 有以下优点：

1. 流式传输：
  HTTP/2 将每一个请求变成流，每一个流都有自己的 ID，有自己的优先级，这些流可以由客户端发送到服务端，也可以由服务端发送到客户端，将数据划分为帧，头部信息为 head 帧，实体信息为 data 帧，最后将这些流乱序发送到一个 TCP 连接中。
2. 并行请求：
  在 HTTP 1.1 中，如果想并发发送多个请求，必须创建多个 TCP 连接。而 HTTP 2 中每个请求的消息由一个或多个帧组成，多个帧之间可以乱序发送，根据帧首部的流标识可以将多个帧重新组装成一个流。这样多个消息可以在同一个 TCP 连接中并行发送。
3. 全双工：
  HTTP 2 中客户端和服务端保持持久的长连接，服务端也可以主动向客户端发送数据。这样就给服务端主动推送事件提供了可能。
4. 长连接：
  HTTP 2 中来自同一个客户端的所有请求都是在单个持久化连接中完成，这个连接可以承载任意数量的双向数据流。

在我们的场景中需要客户端和服务端保持持久的长连接，客户端和服务端在这条连接上双工的发送数据。并且无论服务端、客户端异常断开或重启，长连接都要具备重试保活的能力。使用 HTTP 2 作为我们的底层连接很好的满足了我们需求。

底层连接使用 HTTP 2 的同时需要在应用层具有连接管理的能力，这里选用 gRPC 框架来实现长连接通信协议，提供对底层连接的管理。gRPC 是一个由 google 推出的、高性能、开源、通用的 rpc 框架。它是基于 HTTP 2 协议标准设计开发，默认采用 Protocol Buffers 数据序列化协议，支持多种开发语言。

我们采用 gRPC 的双向流为基础实现通讯层，应用层通过握手协商建立通信，并指定一个连接 id 标志 client 身份，服务通过对连接 id 的识别恢复通信上下文。一般来说除非主动关闭连接，同一个 client 的通信会通过同一条底层 TCP 连接传递，当发生连接抖动或之前的服务器下线时这个 client 的通信会发送给其他的 server，其他 server 通过一致性协议的对账机制恢复上下文继续提供服务。

生命周期感知：

连接的建立：
建立底层 gRPC 连接后应用层协商指定一个连接 id 并初始化上下文

连接的维持：
连接的维持是由 gRPC 提供的心跳来处理的，如果心跳探测失败，底层 grpc 连接会断开，应用层识别到底层连接断开后会进行相应的处理。

主动断开连接：
服务端对客户端的识别是由建立连接时协商的连接 id 指定，所以断开连接只需断开该连接id对应的底层grpc连接即可。gRPC 使用 HTTP 2 协议规定的 GOAWAY 帧信号来控制连接关闭，GOAWAY 用于启动连接关闭或发出严重错误状态信号。GOAWAY 语义为允许端点正常停止接受新的流，同时仍然完成对先前建立的流的处理，当 client 收到这个包之后就会主动关闭连接。下次需要发送数据时，就会重新建立连接。

综上所述，连接层 进行通讯的主要流程如下：

1. 建立底层 TCP 连接  （Client -> Server）
2. 初始化 gRPC 连接，建立 grpc 双向流（HTTP 2 协议协商）
3. 获取连接 ID（应用层握手，一次轻量级 RPC ）
4. 发送初始化连接上下文，连接建立成功
5. 发送业务请求

### 负载均衡功能设计

#### 负载均衡架构

在长连接模式下，每条 TCP 连接仅能接入服务集群中的1台服务器节点，各节点的负载并不均衡，需要动态调整节点数量。由于业务场景不同，长连接的特点也不一样，这里例举三种特点：

1. 每个长连接任务的时间长，短则几天，长则几个月。并且长期占用服务器的资源。
2. 每个长连接任务消耗的资源不一样，比如流量带宽、算力需求的区别，因此每个长连接任务占用的资源大小不一样的。
3. 长连接任务启动时，初始化较慢，需要进行应用层的初始化，例如需要做认证，或是请求一些接口来获取数据。这样会导致做负载均衡时，并不能实时获取到节点的负载情况，出现几秒钟的延时。

根据这些特点有三种比较合适的负载均衡架构模型：

直连模式：

直连模式是部署上最简单，也是最容易理解的一种模式，完全靠客户端来实现负载均衡

![直连模式](http://kirito.iocoder.cn/image-20201224024616439.png)

VIP 模式：

VIP（Virtual IP） 模式可以很好的解决直连模式 IP 变化所带来的应用批量修改的问题。什么是 VIP 呢？

![VIP](http://kirito.iocoder.cn/1567916375212-f3fd5df3-1cc6-4304-aaee-c7bb564e3b79.png)

- Real Server：处理实际请求的后端服务器节点。
- Director Server：指的是负载均衡器节点，负责接收客户端请求，并转发给 RS。
- VIP：Virtual IP，DS 用于和客户端通信的 IP 地址，作为客户端请求的目标 IP 地址。
- DIP：Directors IP，DS 用于和内部 RS 通信的 IP 地址。
- RIP：Real IP，后端服务器的 IP 地址。
- CIP：Client IP，客户端的 IP 地址。

VIP 帮助 Nacos Client 屏蔽了后端 RIP，相对于 RIP 而言，VIP 很少会发生变化。以扩容场景为例，只需要让 VIP 感知到即可，Nacos Client 只需要关注 VIP，避免了扩容引起的代码改造。采用 VIP 模式后，代码不需要感知 RIP

地址服务器模式：

地址服务器模式是对直连模式的一种增强，解决直连模式中无法动态感知集群节点变化的问题。采用这种方式可以完成集群地址和客户端配置的解耦。

地址服务器是用来寻址地址的服务器，客户端发送一个请求，返回后端服务器的地址列表。客户端根据地址服务器返回的列表，随后采取直连模式连接后端服务器。对于客户端来说，只需要知道地址服务器的 ip 即可。

![地址服务器原理](http://kirito.iocoder.cn/image-20201225015919479.png)

这三种方式的总结如下表所示：
|  | 直连模式 | VIP 模式 | 地址服务器模式 |
| --- | --- | --- | --- |
| 转发模式 | 直连 | 代理（网络多一跳） | 直连 |
| 高可用 | 弱，代码配置不灵活，节点故障时无法批量变更 | 强 | 强 |
| 可伸缩性 | 弱 | 强 | 强 |
| 部署成本 | 无 | 负载均衡组件运维成本高 | 地址服务器运维成本低 |
| 负载均衡模式 | nacos-sdk 客户端负载均衡 | 负载均衡组件提供负载均衡能力 | nacos-sdk 客户端负载均衡 |
| 开源接受度 | 高 | 高 | 低，地址服务器模式在开源领域不太普遍 |
| 企业级能力 | 不方便 | 灵活 | 灵活 |
| 跨网络 | 内网环境，平坦网络 | VIP 模式灵活地支持反向代理、安全组、ACL 等特性，可以很好的工作在内/外网环境中，使得应用服务器和 Nacos Server 可以部署在不同的网络环境中，借助 VIP 打通 | 内网环境，平坦网络 |
| 推荐使用环境 | 开发测试环境 | 生产环境，云环境 | 生产环境 |

从高可用和连接层的长连接建立原理的角度考虑只有 VIP 模式和地址服务器模式可以选用：

VIP 模式：
Nginx 与后端服务器建立连接池，长期维持 TCP 长连接，客户端发起请求时与 Nginx 建立长连接，由 Nginx 的 grpc 模块代理请求到后端服务器。由 Nginx进行负载均衡，根据负载均衡策略在连接池中选择一条连接发起请求。整体设计如下图所示：
![图片 1]($resource/%E5%9B%BE%E7%89%87%201.png)
在这种模式下由 Nginx 实现负载均衡算法以指定不同的负载均衡策略。

地址服务器模式：
客户端向地址服务器请求后端服务器的地址列表。客户端根据地址服务器返回的列表，随后使用 P2P 直连模式，使客户端和服务端直接建立连接。整体设计如下图所示：
![图片 2]($resource/%E5%9B%BE%E7%89%87%202.png)
在这种模式下由地址服务器实现负载均衡算法以指定不同的负载均衡策略。

VIP 模式中负载均衡器 Nginx 在数据传输的主链路上，优点是可以支持更灵活的负载均衡策略。缺点是请求经过了一次转发，性能不如直连模式；而且正因为 Nginx 在数据传输的主链路上，这会带来单点问题：如果 Nginx 发生故障即使客户端和服务器都正常整个连接链路也会断掉。

地址服务器模式的优点是客户端和服务端的连接使用 P2P 直连模式性能是最好的，并且地址服务器在请求的旁路上，即使地址服务器故障也不影响现有连接并且客户端也可以通过缓存的服务端 ip 建立新的连接。缺点是支持的负载均衡策略比较有限，并且一但连接建立地址服务器就完全无法干预，无法动态调节服务器的负载。

#### 负载均衡算法

负载均衡这个概念可以抽象为：从 n 个候选服务器中选择一个进行通信的过程。负载均衡算法有多种多样的实现方式，目前常用的负载均衡算法主要有以下几种：

1. 轮询：
  每个请求按时间顺序逐一分配到不同的后端服务，如果后端某台服务器发生故障，自动剔除故障机器，使用户访问不受影响
2. 加权轮询：
  和轮询的原理类似，为每个服务器引入权重的概念。 权重的值越大分配到的访问概率越高，主要用于后端每台服务器性能不均衡的情况下。或者仅仅为在主从的情况下设置不同的权值，达到合理有效的地利用主机资源。
3. 哈希：
  每个请求按访问 IP、客户端 id 或其他固定信息的哈希结果分配，使来自同一个客户端固定访问一台后端服务器
  哈希兼具轮询和加权轮询的全部优点，而且根据哈希算法的不同具有更多的灵活性，本文实现的负载均衡器主要使用哈希算法作为负载均衡算法，下面详细介绍本文使用的哈希算法的实现。

哈希算法又叫散列算法，有很多不同的应用场景，其中最主要的是在密码学中作为门限函数以及作为消息摘要算法使用，在作为消息摘要算法使用时又称摘要算法（Digest），它的作用是：对任意一组输入数据进行计算，得到一个固定长度的输出摘要。哈希算法最重要的特点就是：

- 相同的输入一定得到相同的输出
- 不同的输入大概率得到不同的输出

哈希算法并不是一个特定的算法而是一类算法的统称。可以将他们分成三代：

- 第一代：SHA-1（1993），MD5（1992），CRC（1975），Lookup3（2006）
- 第二代：MurmurHash（2008）
- 第三代：CityHash， SpookyHash（2011）

设计一个哈希算法可以从以下几个角度考虑：

1. 实现复杂程度
2. 分布均匀程度
3. 哈希碰撞概率
4. 性能

本文中的负载均衡器采用一种叫做一致性哈希的哈希算法，一致性哈希负载均衡需要保证的是相同的请求尽可能落到同一个服务器上。在一致性哈希算法的实现上有两个关键点：
1.  什么是相同的请求：一般在使用一致性哈希负载均衡时，需要指定一个 key 用于 hash 计算，可能是：
    1.  请求方 IP
    2.  请求服务名称，参数列表构成的串
    3.  用户 ID

2. 将相同请求落到同一个服务器上：因为服务器可能发生上下线，但是少数服务器的变化不应该影响大多数的请求，所以这个算法需要保证服务器的下线只会影响该服务器处理的请求。这体现了算法名称中的一致性。

综上所述，我们可以概括出一致性哈希负载均衡算法的设计思路。

- 尽可能保证每个服务器节点均匀的分摊流量
- 尽可能保证服务器节点的上下线不影响流量的变更

为此，在传统哈希算法的基础上引入了一致性哈希环的概念：首先将服务器（ip+ 端口号）进行哈希，映射成环上的一个节点，在请求到来时，根据指定的 hash key 同样映射到环上，并顺时针选取最近的一个服务器节点进行请求（在本图中，使用的是 userId 作为 hash key）
[![一致性 hash](https://kirito.iocoder.cn/168f69205ef99590?w=861&h=635&f=png&s=59703)](https://kirito.iocoder.cn/168f69205ef99590?w=861&h=635&f=png&s=59703)

当环上的服务器较少时，即使哈希算法选择得当，依旧会遇到大量请求落到同一个节点的问题，为避免这样的问题，为一致性哈希算法的实现度引入了虚拟节点的概念：即将一个物理服务器转换为多个虚拟节点分散在一致性哈希环上，请求的选址和上述的一致性哈希环保持一致
[![一致性 hash 虚拟节点](https://kirito.iocoder.cn/168f6921775875f4%3Fw=934&h=639&f=png&s=67921.png)](https://kirito.iocoder.cn/168f6921775875f4%3Fw=934&h=639&f=png&s=67921.png)

### 高可用设计

#### 部署模式

本文实现的负载均衡器可以单机部署也可以集群模式部署

单机模式是使用一台机器独立构成集群，在单机模式下集群完全不具备高可用特性，数据无备份、健康检查与告警机制无法开启。单机模式设计上是用于调试代码与验证功能，以及作为演示使用。要启动单机模式只要将集群配置中的 cluster mode 改为 standalone 即可。

集群模式是实现高可用的基石，只有部署多台服务器互为备份才可以在某台服务器故障时可以将请求切换到健康的节点。但是以 VIP 模式部署时因为转发流量的 Nginx 在数据传输的主链路上，会带来单点问题，整个服务的可靠性依赖于 Nginx 服务器的可靠性

#### 健康检查

在分布式的环境下，一个应用启动完毕并不意味着可以对外提供服务，并且一但某个节点发生故障需要具备及时发现、及时摘除流量的能力。负载均衡设备通过健康检查来判断后端服务器的业务可用性。健康检查机制提高了业务整体可用性，避免了后端服务器异常对总体服务造成影响。开启健康检查功能后，当后端某台后端服务器健康检查出现异常时，负载均衡会自动将这台后端服务器的权重值减到零，不再向其发送新的会话，并将新的请求分发到其它健康检查正常的后端服务器上；而当该后端服务器恢复正常运行时，负载均衡会将其自动恢复到负载均衡服务中。

健康检查机制的引入有效提高了业务服务的可用性。但是为了避免频繁的健康检查失败引起的切换对系统可用性的冲击，健康检查只有在健康检查时间窗内连续多次检查成功或失败后，才会进行状态切换。健康检查时间窗由以下三个因素决定：

- 健康检查间隔 (每隔多久进行一次健康检查)
- 响应超时时间 (等待服务器返回健康检查的时间)
- 检查阈值 (健康检查连续成功或失败的次数)

健康检查时间窗的计算方法如下：

- 健康检查失败时间窗=响应超时时间×不健康阈值+检查间隔×(不健康阈值-1)

注：如果健康检查直接收到 TCP 的 RST 报文，则不需要等待，直接进行下一次检查。

![健康检查流程](https://cdn.nlark.com/lark/0/2018/png/67321/1545017738669-ce894eed-0403-49ef-ba0e-b193b2bb0543.png)

#### 告警设计

上一节介绍了长连接负载均衡器具备的健康检查能力，当检查到后端服务器 ip 不健康时可以自动摘除相应 ip 的流量，当 ip 健康状态恢复时自动恢复流量。但是 ip 被摘除和恢复并没有通知用户，这会带来两个问题：

1. 用户很难感知：只有在排查问题时发现客户端获取的 ip 列表没有相应的 ip 或手动登陆控制台查看 ip 的健康状态才能发现 ip 被摘除
2. 健康检查不够精确：特别是默认选择的 TCP 端口探测并不能准确反映业务的可用性，在网络波动等情况需要提醒用户确认

所以，长连接负载均衡器需要具备告警功能，在 ip 健康状态改变时通知用户流量被摘除或恢复，提醒用户进行故障检查。

为最小化降低对核心功能的影响，告警功能的实现做在旁路上，全部采用异步机制。长连接负载均衡器的健康检查机制是集群中的每台机器无重复的负责对一部分机器的检查。所以告警功能也可以切分到集群中的每台机器负责发送自己检查机器的告警信息。主要思路是利用长连接负载均衡器的健康检查能力，将 vipserver 检查出健康状态改变的 ip 列表缓存到内存的 cache 中，由告警模块定时拉取。cache 的数据结构选用 HashMap，每条记录的 key 为“机器名+ip”，这样极限情况下数据量相当于系统中 ip 的数量，因为每条记录的 value 是 ip 的引用，所以对内存的消耗量不大，可以最大程度避免对抢占核心功能资源，减少对系统的压力。

发送告警的步骤如下：

1. 定时（默认为 1 分钟）异步拉取 cache 中的 ip 状态改变事件
2. 根据既定的发送规则筛选出需要发送的事件
3. 将事件聚合到域名维度
4. 调用接口对每个域名发送告警

![image.png](https://intranetproxy.alipay.com/skylark/lark/0/2021/png/305469/1619775961886-35aa565d-18bf-4535-a454-bf4e0fee30a2.png)

## 长连接负载均衡器的功能测试与性能分析

### 单元测试

单元测试（Unit Testing）又称为模块测试，是针对软件设计的最小单位程序模块进行正确性检验的测试工作。在过程化编程中，一个单元就是单个程序、函数、过程等；对于面向对象编程，最小单元就是方法，包括基类（超类）、抽象类、或者派生类（子类）中的方法。单元测试可以总结为测试对象是独立的代码最小单元的逻辑，并且不依赖环境的测试。

单元测试的意义如下：

- 单元测试是一切测试进行的基石，它针对最小粒度unit的测试，位于测试金字塔的最底层，再往上是Service（比如淘宝体系下的HSF接口测试）－UI（比如基于webdriver的UI自动化测试），在实践的过程中你会发现单元测色是对代码进行测试的最有效工具之一
- 有更快的开发－验证循环（开发过程被划分为更小的快速迭代过程），无需等待依赖实现，无需特定环境
- 节省调试时间（越复杂的功能，潜在收益越大）
- 帮助应对业务的快速变化，用于回归验证
- 便于项目交接或多人协作

本文实现的负载均衡器中的单元测试是对每个功能模块以类为单位，对核心方法撰写单元测试代码。本项目的单元测试覆盖率达到 68%，基本覆盖全部的核心功能。

### 功能测试

功能测试的意义在于测试负载均衡功能能否正常使用，预期的能力是否能够实现。功能测试包含的内容首先是两个核心模块通讯层、负载均衡模块的功能是否正常，其次是测试整个系统是否实现了预期的高可用能力。

#### 测试通讯层功能

通讯层的测试包括以下几项：

1. 连接管理功能是否正常：能否成功建立并持有 TCP 长连接，能否主动断开连接或响应连接断开事件
2. 应用层数据是否能正常传输
3. 事件推送能否成功，客户端能否正常响应

以单机模式启动一台服务端然后用 10 个客户端去连接这台服务端，客户端并发的向服务器发送请求。请求的内容是一个测试接口，这个测试接口是用于打印当前连接的详细信息，由此能够验证服务端能否正常响应用户请求并取出持有的相应连接完成响应。然后客户端调用用于关闭连接的接口，查看服务端日志验证连接是否正确关闭，检测服务端是否能正确关闭连接。接下来服务端发送关闭连接的事件，验证事件推送功能是否正常工作，客户端能否正常响应连接关闭事件。最后将未关闭的连接保持连接状态 1 天，验证客户端能否靠心跳保活、服务端能否长时间持有 TCP 长连接。

经过建立测试集群并编写测试用例在拟真环境中测试，最终得出结论本文实现的负载均衡器很好的完成了既定的功能

#### 测试负载均衡功能

负载均衡模块的测试包括以下几项：

1. vip 和域名服务器这两种负载均衡架构能否正常工作
2. 一致性哈希算法能否正常工作，TCP 长连接是否按照预定的权重分配给后端服务器

搭建 VIP 模式和地址服务器模式的负载均衡器以及后端服务器集群，客户端经由负载均衡器访问服务端，查看服务端能否正常给出回应，验证这两种模式的负载均衡器能否正常工作。在负载均衡器上配置每台后端服务器有不同的权重，然后开启 200 个客户端同时并发的连接服务端，查看每台服务端的日志比对当前的连接数占中连接数的比例是否和配置的权重吻合，验证负载均衡器的负载均衡算法是否正常工作

经过建立测试集群并使用大量客户端反复连接集群，最终得出结论本文实现的负载均衡器能够实现预期的负载均衡功能

#### 测试高可用功能

高可用功能的测试包括以下几项：

1. 健康检查能力能否及时检查出不健康的机器
2. 健康状态改变的事件能否及时发出告警通知
3. 负载均衡器能否将不健康的机器流量摘除
4. 当不健康的机器恢复健康时负载均衡器能否给这些机器恢复流量

搭建本文实现的负载均衡器和后端服务器集群，然后启动 100 个客户端连接到负载均衡器集群上并定时循环发送请求，然后随机下线一个服务端，查看现有存量客户端的连接断开后是否能向其他的服务端建立连接，增量的新客户端是否还会向不健康的节点建立连接，依次验证健康检查功能是否生效、负载均衡器能否摘除不健康节点的流量。然后将这个节点重新启动，节点上线后查看是否有新的连接建立，验证负载均衡器能否恢复解除故障重新上线的节点的流量

经过反复的上下线测试，负载均衡服务器能够快速准确的摘除和恢复流量，并且能够及时发出告警通知

### 性能压测

性能压测的主要目的是找出目前的性能瓶颈并予以针对性优化和的，性能压测主要分为连接层和负载均衡模块两个部分

#### 连接层模块压测

连接层压测主要目的是找出单机能够承载的 TPS 极限和性能瓶颈。以单机模式启动一台服务端，然后用数台试压机启动多个客户端连接到服务端发送请求，通过预先埋点和监控操作系统资源的方式统计 TPS、RT、CPU 占用率等信息。压测报告见下表：

| 施压机数量 | 每台线程数 | 平均TPS | 平均RT | 最小RT | 最大RT | 80%RT(ms) | 95%RT(ms) | 99%RT(ms) | CPU使用率 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 1 | 100 | 12998.46 | 7.54 | 0.55 | 213.86 | 9.68 | 10.69 | 27.92 | 40% |
| 2 | 50 | 12785.01 | 7.93 | 0.38 | 900.48 | 8.34 | 14.18 | 33.04 | 40% |
| 2 | 100 | 18451.78 | 10.63 | 0.6 | 829.42 | 11.95 | 23.79 | 44.19 | 45% |
| 5 | 20 | 30680.48 | 3.12 | 0.46 | 1138.38 | 4.33 | 5.9 | 9.57 | 50% |

#### 负载均衡模块压测

负载均衡模块压测的主要目的是找出负载均衡模块能够承载的连接数上限，这一压测仅对于 VIP 模式的负载均衡器进行，因为域名服务器模式的负载均衡器仅仅在客户端初始化时提供后端服务器的 ip 列表，客户端随后采用 P2P 直连的方式与服务端建立连接。理论上域名服务器模式下的后端服务器可以无限水平扩展，所以对其进行压测是毫无意义的。

搭建 VIP 模式的负载均衡器以及 3 台后端服务器组成集群，然后用数台试压机启动多个客户端由负载均衡器连接到服务端发送请求，通过预先埋点和监控操作系统资源的方式统计 TPS、RT、CPU 占用率等信息。压测报告见下表：

| 施压机数量 | 每台线程数 | 平均TPS | 平均RT | 最小RT | 最大RT | 80%RT(ms) | 95%RT(ms) | 99%RT(ms) | CPU使用率 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 1 | 100 | 7256.32 | 13.14 | 0.39 | 2522.25 | 6.72 | 12.86 | 126.33 | 80% |
| 2 | 50 | 16418.04 | 5.8 | 0.41 | 3906.77 | 4.0 | 8.88 | 48.84 | 90% |
| 5 | 20 | 26784.84 | 3.6 | 0.38 | 1606.41 | 3.82 | 8.91 | 30.62 | 90% |

对比单机连接层压测报告可以看出，经过 VIP 模式的负载均衡器转发流量到后端服务器对性能是有很大影响的。3 台后端服务器对比单机连接层压测报告只有在两台施压机每台线程数为 50 的情况下有 TPS 的提升，其他情况下反而出现了性能的下降。

## 总结与展望

本文实现的基于 TCP 长连接的负载均衡器很好完成了预期的目标：

基于 TCP 长连接实现通讯层对比短连接性能提升十倍。支持流式传输、并行请求、全双工、长连接等高级特性。能够实现对建立连接、维持连接、断开连接这一整个生命周期的管理和感知。具备事件推送的能力。

负载均衡模块能够很好的对 TCP 长连接进行负载均衡。提供基于 VIP 和域名服务器的两种部署方式。前者支持更好更灵活的负载均衡算法，并且基于 Nginx 开源项目支持增加鉴权等额外组件；后者在提供负载均衡功能的同时对性能毫无影响，能够最大程度的发挥通讯层长连接的强大性能。同时负载均衡模块具备高可用能力，借助健康检查功能快速发现节点故障并摘除流量，在故障节点恢复正常后能够恢复流量。

在开发和进行压测的过程中，作者发现当前实现的负载均衡器还有一些不足之处：

1. 健康检查并不能准确反映服务器的可用情况：
  VIP 模式下的负载均衡器的健康检查因为是处于主链路上的，只要简单的监测和后端服务器的连接状态即可。基于地址服务器模式下的负载均衡器的健康检查目前是处于旁路上，通过 TCP 端口探测来检测服务是否可用。这种健康检查机制并不十分可靠，比如因为 CPU 占用过高服务无法响应，在这种情况下端口探测正常但是服务以及不可用了；地址服务器与后端服务器发生网络分区，但是客户端和服务器之间网络正常，在这种情况下负载均衡器认为服务不可用但是实际上服务仍然是可用的。
2. 已经建立的连接不能柔性调节：
  本文实现的负载均衡器对已经建立的连接没有任何的干预能力。如果一台故障的服务器重启上线或者有新的机器加入集群，此时只有新建的连接能够按照权重与其进行连接，其他存量连接无法转移到这台新服务器上，这就造成了连接分配的不均衡。
3. VIP 模式下负载均衡器存在单点问题和性能瓶颈：
  VIP 模式中负载均衡器 Nginx 在数据传输的主链路上，请求经过了一次转发，理论上性能不如直连模式，通过压测报告的结论来看对连接层的性能有很大影响；而且正因为 Nginx 在数据传输的主链路上，会带来单点问题：如果 Nginx 发生故障即使客户端和服务器都正常整个连接链路也会断掉。
